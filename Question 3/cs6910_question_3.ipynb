{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3:\n",
        "\n",
        "Implement the backpropagation algorithm with support for the following optimisation functions\n",
        "\n",
        "*   sgd\n",
        "*   momentum based gradient descent\n",
        "*   nesterov accelerated gradient descent\n",
        "*   rmsprop\n",
        "*   adam\n",
        "*   nadam\n",
        "\n",
        "(12 marks for the backpropagation framework and 2 marks for each of the optimisation algorithms above)\n",
        "\n",
        "We will check the code for implementation and ease of use (e.g., how easy it is to add a new optimisation algorithm such as Eve). Note that the code should be flexible enough to work with different batch sizes."
      ],
      "metadata": {
        "id": "R4ULXAST2hLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importing the FASHION-MNIST dataset**\n",
        "\n",
        "**Furthermore, the data is segreggated into training and test data**"
      ],
      "metadata": {
        "id": "KilU0eW09lzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "X,Y = train_images.reshape(train_images.shape[0], -1).T/225,train_labels\n",
        "X_test,Y_test = test_images.reshape(test_images.shape[0], -1).T/225,test_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65TM9UYH9iBO",
        "outputId": "858d749e-7714-4077-d123-6754692ebf59"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting a part of it for validation:**"
      ],
      "metadata": {
        "id": "oWc667y7PVxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_ratio = 0.1 #percentage of data for validation\n",
        "num_validation_samples = int(validation_ratio * X.shape[1])\n",
        "indices = np.random.permutation(X.shape[1]) #shuffling the indices\n",
        "validation_indices = indices[:num_validation_samples]\n",
        "training_indices = indices[num_validation_samples:]\n",
        "X_train = X[:, training_indices]\n",
        "Y_train = Y[training_indices]\n",
        "X_val = X[:, validation_indices]\n",
        "Y_val = Y[validation_indices]"
      ],
      "metadata": {
        "id": "Sir9Yue0PX40"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing a few functions that we have defined, for example, activation functions, their derrivatives, intializers etc.**"
      ],
      "metadata": {
        "id": "cYoK3QZ0_UML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from support_functions import *\n",
        "from activation_functions import *\n",
        "from initializers import *"
      ],
      "metadata": {
        "id": "Q9UBnalxUbD8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Forward proagartion function:**"
      ],
      "metadata": {
        "id": "n_EacCBEWnAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from forward_propagation import forward_prop #Question 2"
      ],
      "metadata": {
        "id": "CBbKjwCPWpln"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Loss function:**"
      ],
      "metadata": {
        "id": "0rBX_VaQVR8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(Y_pred, Y, epsilon, params, lambd):\n",
        "    m = Y.shape[0]\n",
        "    one_hot_Y = one_hot(Y)\n",
        "    cross_entropy_loss = -np.mean(one_hot_Y * np.log(Y_pred + epsilon))\n",
        "\n",
        "    # Computing L2 regularization term\n",
        "    l2_regularization = 0\n",
        "    num_layers = len(params) // 2\n",
        "    for i in range(1, num_layers + 1):\n",
        "        l2_regularization += np.sum(np.square(params[f'W{i}']))\n",
        "\n",
        "    l2_regularization *= (lambd / (2 * m))\n",
        "\n",
        "    total_loss = cross_entropy_loss + l2_regularization\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "uesgPV1LVUlD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 3 (part-A): Backward Propagation**"
      ],
      "metadata": {
        "id": "Rdqs1kCMVA3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_prop(params, activations, X, Y, act, lambd):\n",
        "    m = Y.size\n",
        "    num_layers = len(params) // 2\n",
        "    one_hot_Y = one_hot(Y)\n",
        "\n",
        "    if act == 'sigmoid':\n",
        "        grad_activation = grad_sigmoid\n",
        "    elif act == 'relu':\n",
        "        grad_activation = grad_ReLU\n",
        "    elif act == 'tanh':\n",
        "        grad_activation = grad_tanh\n",
        "\n",
        "    gradients = {}\n",
        "\n",
        "    # Backpropagate through output layer\n",
        "    da_last = - (one_hot_Y - activations[f'h{num_layers}'])\n",
        "    dw_last = 1 / m * np.dot(da_last, activations[f'h{num_layers-1}'].T) + (lambd / m) * params[f'W{num_layers}']\n",
        "    db_last = 1 / m * np.sum(da_last, axis=1, keepdims=True)\n",
        "    gradients[f'dW{num_layers}'] = dw_last\n",
        "    gradients[f'db{num_layers}'] = db_last\n",
        "\n",
        "    # Backpropagate through hidden layers\n",
        "    da = da_last\n",
        "    for i in reversed(range(1, num_layers)):\n",
        "        da = np.dot(params[f'W{i+1}'].T, da) * grad_activation(activations[f'a{i}'])\n",
        "\n",
        "        dw = 1 / m * np.dot(da, activations[f'h{i-1}'].T) + (lambd / m) * params[f'W{i}']\n",
        "        db = 1 / m * np.sum(da, axis=1, keepdims=True)\n",
        "        gradients[f'dW{i}'] = dw\n",
        "        gradients[f'db{i}'] = db\n",
        "\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "43J7LZxxVHf8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 3 (part-B):Gradient-descent algorithms**"
      ],
      "metadata": {
        "id": "ujzkzImxVcuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(weight, activation, weight_decay, X, Y, X_val, Y_val, iterations, alpha, layer_sizes, batch_size):\n",
        "    epsilon = 0.\n",
        "    if weight=='random':\n",
        "      params = init_params(layer_sizes)\n",
        "    elif weight=='glorot':\n",
        "      params = xavier_init_params(layer_sizes)\n",
        "    else:\n",
        "      print(\"Please choose glorot or random initialization\")\n",
        "\n",
        "    num_batches = len(X.T) // batch_size\n",
        "\n",
        "    for j in range(iterations):\n",
        "\n",
        "      for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        X_batch = X[:,start_idx:end_idx]\n",
        "        Y_batch = Y[start_idx:end_idx]\n",
        "        activations = forward_prop(params, X_batch, activation)\n",
        "        train_loss = compute_loss(activations[f'h{len(layer_sizes)-1}'], Y_batch, epsilon, params, weight_decay)\n",
        "\n",
        "        activations_val = forward_prop(params, X_val, activation)\n",
        "        val_loss = compute_loss(activations_val[f'h{len(layer_sizes)-1}'], Y_val, epsilon, params, weight_decay)\n",
        "\n",
        "        gradients = backward_prop(params, activations, X_batch, Y_batch, activation, weight_decay)\n",
        "        params = update_params(params, gradients, alpha)\n",
        "\n",
        "      print(\"Epoch: \" + str(j+1) + \"/\" + str(iterations) + \"; Batch:  \" + str(i+1) + \"/\" + str(num_batches) + \"; Train Loss: \" + str(train_loss) + \"; Val Loss: \" + str(val_loss))\n",
        "\n",
        "    return params\n",
        "\n",
        "#momentum-gradient-descent\n",
        "def momentum_gradient_descent(weight, activation, weight_decay, X, Y, X_val, Y_val, iterations, alpha, beta, layer_sizes, batch_size):\n",
        "    epsilon = 0.\n",
        "    if weight=='random':\n",
        "      params = init_params(layer_sizes)\n",
        "    elif weight=='glorot':\n",
        "      params = xavier_init_params(layer_sizes)\n",
        "    else:\n",
        "      print(\"Please choose glorot or random initialization\")\n",
        "\n",
        "    history_prev = initialize_history(params)\n",
        "    num_batches = len(X.T) // batch_size\n",
        "\n",
        "    for j in range(iterations):\n",
        "      for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        X_batch = X[:,start_idx:end_idx]\n",
        "        Y_batch = Y[start_idx:end_idx]\n",
        "        activations_val = forward_prop(params, X_val, activation)\n",
        "        val_loss = compute_loss(activations_val[f'h{len(layer_sizes)-1}'], Y_val, epsilon, params, weight_decay)\n",
        "\n",
        "        activations = forward_prop(params, X_batch, activation)\n",
        "        train_loss = compute_loss(activations[f'h{len(layer_sizes)-1}'], Y_batch, epsilon, params, weight_decay)\n",
        "        gradients = backward_prop(params, activations, X_batch, Y_batch, activation, weight_decay)\n",
        "\n",
        "        history = add_dicts(gradients, history_prev, scalar2=beta)\n",
        "        params = update_momentum_params(params, history, alpha)\n",
        "        history_prev=history\n",
        "\n",
        "      print(\"Epoch: \" + str(j+1) + \"/\" + str(iterations) + \"; Batch:  \" + str(i+1) + \"/\" + str(num_batches) + \"; Train Loss: \" + str(train_loss) + \"; Val Loss: \" + str(val_loss))\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "#nesterov-gradient-descent\n",
        "def nesterov_gradient_descent(weight, activation, weight_decay, X, Y, X_val, Y_val,iterations, alpha, beta, layer_sizes, batch_size):\n",
        "    epsilon = 0.\n",
        "    if weight=='random':\n",
        "      params = init_params(layer_sizes)\n",
        "    elif weight=='glorot':\n",
        "      params = xavier_init_params(layer_sizes)\n",
        "    else:\n",
        "      print(\"Please choose glorot or random initialization\")\n",
        "\n",
        "    history_prev = initialize_history(params)\n",
        "    num_batches = len(X.T) // batch_size\n",
        "\n",
        "    for j in range(iterations):\n",
        "      for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        X_batch = X[:,start_idx:end_idx]\n",
        "        Y_batch = Y[start_idx:end_idx]\n",
        "        activations_val = forward_prop(params, X_val, activation)\n",
        "        val_loss = compute_loss(activations_val[f'h{len(layer_sizes)-1}'], Y_val, epsilon, params, weight_decay)\n",
        "\n",
        "        activations = forward_prop(params, X_batch, activation)\n",
        "        train_loss = compute_loss(activations[f'h{len(layer_sizes)-1}'], Y_batch, epsilon, params, weight_decay)\n",
        "\n",
        "        nesterov_params = nesterov_params_look(params, history_prev, beta*alpha)\n",
        "        nesterov_activations = forward_prop(nesterov_params, X_batch, activation)\n",
        "        nesterov_gradients = backward_prop(nesterov_params, nesterov_activations, X_batch, Y_batch, activation, weight_decay)\n",
        "        history = add_dicts(nesterov_gradients, history_prev, scalar2=beta)\n",
        "        params = update_momentum_params(params, history, alpha)\n",
        "\n",
        "        history_prev=history\n",
        "\n",
        "      print(\"Epoch: \" + str(j+1) + \"/\" + str(iterations) + \"; Batch:  \" + str(i+1) + \"/\" + str(num_batches) + \"; Train Loss: \" + str(train_loss) + \"; Val Loss: \" + str(val_loss))\n",
        "\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "#sto-gradient-descent-with-adagrad\n",
        "def stochastic_gradient_descent_with_adagrad(weight, activation, weight_decay, X, Y, X_val, Y_val,iterations, alpha, layer_sizes, batch_size, epsilon_v):\n",
        "    epsilon = 0.\n",
        "    if weight=='random':\n",
        "      params = init_params(layer_sizes)\n",
        "    elif weight=='glorot':\n",
        "      params = xavier_init_params(layer_sizes)\n",
        "    else:\n",
        "      print(\"Please choose glorot or random initialization\")\n",
        "\n",
        "    num_batches = len(X.T) // batch_size\n",
        "    v_old = initialize_history(params)\n",
        "\n",
        "    for j in range(iterations):\n",
        "\n",
        "      for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        X_batch = X[:,start_idx:end_idx]\n",
        "        Y_batch = Y[start_idx:end_idx]\n",
        "        activations = forward_prop(params, X_batch, activation)\n",
        "        train_loss = compute_loss(activations[f'h{len(layer_sizes)-1}'], Y_batch, epsilon, params, weight_decay)\n",
        "\n",
        "        activations_val = forward_prop(params, X_val, activation)\n",
        "        val_loss = compute_loss(activations_val[f'h{len(layer_sizes)-1}'], Y_val, epsilon, params, weight_decay)\n",
        "\n",
        "        gradients = backward_prop(params, activations, X_batch, Y_batch, activation, weight_decay)\n",
        "        gradients_squared = {key: np.square(value) for key, value in gradients.items()}\n",
        "        v = add_dicts(gradients_squared,v_old)\n",
        "        params = update_params_adagrad(params, gradients, alpha, v, epsilon_v)\n",
        "        v_old = v\n",
        "      print(\"Epoch: \" + str(j+1) + \"/\" + str(iterations) + \"; Batch:  \" + str(i+1) + \"/\" + str(num_batches) + \"; Train Loss: \" + str(train_loss) + \"; Val Loss: \" + str(val_loss))\n",
        "\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "#sto-gradient-descent-with-rmsprop\n",
        "def stochastic_gradient_descent_with_RMSProp(weight, activation, weight_decay, X, Y, X_val, Y_val, iterations, alpha, layer_sizes, batch_size, epsilon_v, beta):\n",
        "    epsilon = 0.\n",
        "    if weight=='random':\n",
        "      params = init_params(layer_sizes)\n",
        "    elif weight=='glorot':\n",
        "      params = xavier_init_params(layer_sizes)\n",
        "    else:\n",
        "      print(\"Please choose glorot or random initialization\")\n",
        "\n",
        "    num_batches = len(X.T) // batch_size\n",
        "    v_old = initialize_history(params)\n",
        "\n",
        "    for j in range(iterations):\n",
        "\n",
        "      for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        X_batch = X[:,start_idx:end_idx]\n",
        "        Y_batch = Y[start_idx:end_idx]\n",
        "        activations = forward_prop(params, X_batch, activation)\n",
        "        train_loss = compute_loss(activations[f'h{len(layer_sizes)-1}'], Y_batch, epsilon, params, weight_decay)\n",
        "\n",
        "        activations_val = forward_prop(params, X_val, activation)\n",
        "        val_loss = compute_loss(activations_val[f'h{len(layer_sizes)-1}'], Y_val, epsilon, params, weight_decay)\n",
        "\n",
        "        gradients = backward_prop(params, activations, X_batch, Y_batch, activation, weight_decay)\n",
        "        gradients_squared = {key: np.square(value) for key, value in gradients.items()}\n",
        "\n",
        "        v = add_dicts(gradients_squared,v_old,scalar1=(1-beta),scalar2=beta)\n",
        "        params = update_params_adagrad(params, gradients, alpha, v, epsilon_v)\n",
        "        v_old = v\n",
        "      print(\"Epoch: \" + str(j+1) + \"/\" + str(iterations) + \"; Batch:  \" + str(i+1) + \"/\" + str(num_batches) + \"; Train Loss: \" + str(train_loss) + \"; Val Loss: \" + str(val_loss))\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "#sto-gradient-descent-with-adadelta\n",
        "def stochastic_gradient_descent_with_AdaDelta(weight, activation, weight_decay, X, Y, X_val, Y_val, iterations, alpha, layer_sizes, batch_size, epsilon_v, beta):\n",
        "    epsilon = 0.\n",
        "    if weight=='random':\n",
        "      params = init_params(layer_sizes)\n",
        "    elif weight=='glorot':\n",
        "      params = xavier_init_params(layer_sizes)\n",
        "    else:\n",
        "      print(\"Please choose glorot or random initialization\")\n",
        "\n",
        "    num_batches = len(X.T) // batch_size\n",
        "    v_old = initialize_history(params)\n",
        "    u_old = initialize_history(params)\n",
        "\n",
        "    for j in range(iterations):\n",
        "\n",
        "      for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        X_batch = X[:,start_idx:end_idx]\n",
        "        Y_batch = Y[start_idx:end_idx]\n",
        "        activations = forward_prop(params, X_batch, activation)\n",
        "        train_loss = compute_loss(activations[f'h{len(layer_sizes)-1}'], Y_batch, epsilon, params, weight_decay)\n",
        "\n",
        "        activations_val = forward_prop(params, X_val, activation)\n",
        "        val_loss = compute_loss(activations_val[f'h{len(layer_sizes)-1}'], Y_val, epsilon, params, weight_decay)\n",
        "\n",
        "        gradients = backward_prop(params, activations, X_batch, Y_batch, activation, weight_decay)\n",
        "        gradients_squared = {key: np.square(value) for key, value in gradients.items()}\n",
        "        v = add_dicts(gradients_squared,v_old,scalar1=(1-beta),scalar2=beta)\n",
        "        delw = del_w(u_old,v,gradients,epsilon_v)\n",
        "        params = update_params_adadelta(params, delw)\n",
        "        u = add_dicts(gradients_squared,u_old,scalar1=(1-beta),scalar2=beta)\n",
        "\n",
        "        v_old = v\n",
        "        u_old = u\n",
        "      print(\"Epoch: \" + str(j+1) + \"/\" + str(iterations) + \"; Batch:  \" + str(i+1) + \"/\" + str(num_batches) + \"; Train Loss: \" + str(train_loss) + \"; Val Loss: \" + str(val_loss))\n",
        "\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "\n",
        "#sto-gradient-descent-with-adam\n",
        "def stochastic_gradient_descent_with_adam(weight, activation, weight_decay, X, Y, X_val, Y_val,iterations, alpha, layer_sizes, batch_size, epsilon_v, beta1, beta2):\n",
        "    epsilon = 0.\n",
        "    if weight=='random':\n",
        "      params = init_params(layer_sizes)\n",
        "    elif weight=='glorot':\n",
        "      params = xavier_init_params(layer_sizes)\n",
        "    else:\n",
        "      print(\"Please choose glorot or random initialization\")\n",
        "\n",
        "    num_batches = len(X.T) // batch_size\n",
        "    m_old = initialize_history(params)\n",
        "    v_old = initialize_history(params)\n",
        "\n",
        "    for j in range(iterations):\n",
        "\n",
        "      for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        X_batch = X[:,start_idx:end_idx]\n",
        "        Y_batch = Y[start_idx:end_idx]\n",
        "        activations = forward_prop(params, X_batch, activation)\n",
        "        train_loss = compute_loss(activations[f'h{len(layer_sizes)-1}'], Y_batch, epsilon,params, weight_decay)\n",
        "        activations_val = forward_prop(params, X_val, activation)\n",
        "        val_loss = compute_loss(activations_val[f'h{len(layer_sizes)-1}'], Y_val, epsilon, params, weight_decay)\n",
        "        gradients = backward_prop(params, activations, X_batch, Y_batch, activation, weight_decay)\n",
        "        gradients_squared = {key: np.square(value) for key, value in gradients.items()}\n",
        "        m = add_dicts(gradients,m_old,scalar1=(1-beta1),scalar2=beta1)\n",
        "        m_bar = dict_div(m,(1-beta1))\n",
        "        v = add_dicts(gradients_squared,v_old,scalar1=(1-beta2),scalar2=beta2)\n",
        "        v_bar = dict_div(v,(1-beta2))\n",
        "        params = update_params_adam(params, m_bar, v_bar, epsilon_v, alpha)\n",
        "        v_old = v\n",
        "        m_old = m\n",
        "      print(\"Epoch: \" + str(j+1) + \"/\" + str(iterations) + \"; Batch:  \" + str(i+1) + \"/\" + str(num_batches) + \"; Train Loss: \" + str(train_loss) + \"; Val Loss: \" + str(val_loss))\n",
        "\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "\n",
        "#sto-gradient-descent-with-nadam\n",
        "def stochastic_gradient_descent_with_nadam(weight, activation, weight_decay, X, Y, X_val, Y_val, iterations, alpha, layer_sizes, batch_size, epsilon_v, beta1, beta2):\n",
        "    epsilon = 0.\n",
        "    if weight=='random':\n",
        "      params = init_params(layer_sizes)\n",
        "    elif weight=='glorot':\n",
        "      params = xavier_init_params(layer_sizes)\n",
        "    else:\n",
        "      print(\"Please choose glorot or random initialization\")\n",
        "\n",
        "    num_batches = len(X.T) // batch_size\n",
        "    m_old = initialize_history(params)\n",
        "    v_old = initialize_history(params)\n",
        "\n",
        "    for j in range(iterations):\n",
        "\n",
        "      for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = (i + 1) * batch_size\n",
        "        X_batch = X[:,start_idx:end_idx]\n",
        "        Y_batch = Y[start_idx:end_idx]\n",
        "        activations = forward_prop(params, X_batch, activation)\n",
        "        train_loss = compute_loss(activations[f'h{len(layer_sizes)-1}'], Y_batch, epsilon,params,weight_decay)\n",
        "        activations_val = forward_prop(params, X_val, activation)\n",
        "        val_loss = compute_loss(activations_val[f'h{len(layer_sizes)-1}'], Y_val, epsilon, params, weight_decay)\n",
        "        gradients = backward_prop(params, activations, X_batch, Y_batch, activation, weight_decay)\n",
        "        gradients_squared = {key: np.square(value) for key, value in gradients.items()}\n",
        "        m = add_dicts(gradients,m_old,scalar1=(1-beta1),scalar2=beta1)\n",
        "        m_bar = dict_div(m,(1-beta1))\n",
        "        v = add_dicts(gradients_squared,v_old,scalar1=(1-beta2),scalar2=beta2)\n",
        "        v_bar = dict_div(v,(1-beta2))\n",
        "        bracterm = add_dicts(m,gradients,scalar1=beta1,scalar2=1.)\n",
        "        params = update_params_adam(params, bracterm, v_bar, epsilon_v, alpha)\n",
        "        v_old = v\n",
        "        m_old = m\n",
        "      print(\"Epoch: \" + str(j+1) + \"/\" + str(iterations) + \"; Batch:  \" + str(i+1) + \"/\" + str(num_batches) + \"; Train Loss: \" + str(train_loss) + \"; Val Loss: \" + str(val_loss))\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "9aX1FOuxVKDI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 3 (part-C):Updating parameters, part of gradient-descent**"
      ],
      "metadata": {
        "id": "X6P4kBR1Vst4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vanill_update_parameters\n",
        "def update_params(params, gradients, alpha):\n",
        "    updated_params = {}\n",
        "    num_layers = len(params) // 2\n",
        "    for i in range(1, num_layers + 1):\n",
        "        updated_params[f'W{i}'] = params[f'W{i}'] - alpha * gradients[f'dW{i}']\n",
        "        updated_params[f'b{i}'] = params[f'b{i}'] - alpha * gradients[f'db{i}']\n",
        "    return updated_params\n",
        "\n",
        "#adagrad_update_parameters\n",
        "def update_params_adagrad(params, gradients, alpha, v, epsilon):\n",
        "    updated_params = {}\n",
        "    num_layers = len(params) // 2\n",
        "    for i in range(1, num_layers + 1):\n",
        "        updated_params[f'W{i}'] = params[f'W{i}'] - alpha/np.sqrt(v[f'dW{i}']+epsilon) * gradients[f'dW{i}']\n",
        "        updated_params[f'b{i}'] = params[f'b{i}'] - alpha/np.sqrt(v[f'db{i}']+epsilon) * gradients[f'db{i}']\n",
        "    return updated_params\n",
        "\n",
        "#nesterov_update_parameters\n",
        "def update_nesterov_params(params, history, alpha):\n",
        "    updated_params = {}\n",
        "    num_layers = len(params) // 2\n",
        "    for i in range(1, num_layers + 1):\n",
        "        updated_params[f'W{i}'] = params[f'W{i}'] - alpha * history[f'W{i}']\n",
        "        updated_params[f'b{i}'] = params[f'b{i}'] - alpha * history[f'b{i}']\n",
        "    return updated_params\n",
        "\n",
        "#adadelta_update_parameters\n",
        "def update_params_adadelta(params, delw):\n",
        "    updated_params = {}\n",
        "    num_layers = len(params) // 2\n",
        "    for i in range(1, num_layers + 1):\n",
        "        updated_params[f'W{i}'] = params[f'W{i}'] + delw[f'dW{i}']\n",
        "        updated_params[f'b{i}'] = params[f'b{i}'] + delw[f'db{i}']\n",
        "    return updated_params\n",
        "\n",
        "#momentum_update_parameters\n",
        "def update_momentum_params(params, history, alpha):\n",
        "    updated_params = {}\n",
        "    num_layers = len(params) // 2\n",
        "    for i in range(1, num_layers + 1):\n",
        "        updated_params[f'W{i}'] = params[f'W{i}'] - alpha * history[f'dW{i}']\n",
        "        updated_params[f'b{i}'] = params[f'b{i}'] - alpha * history[f'db{i}']\n",
        "    return updated_params\n",
        "\n",
        "#adam_update_parameters\n",
        "def update_params_adam(params, m_bar, v_bar, epsilon, alpha):\n",
        "    updated_params = {}\n",
        "    num_layers = len(params) // 2\n",
        "    for i in range(1, num_layers + 1):\n",
        "        updated_params[f'W{i}'] = params[f'W{i}'] - alpha/(np.sqrt(v_bar[f'dW{i}'])+epsilon) *   m_bar[f'dW{i}']\n",
        "        updated_params[f'b{i}'] = params[f'b{i}'] - alpha/(np.sqrt(v_bar[f'db{i}'])+epsilon) *   m_bar[f'db{i}']\n",
        "    return updated_params\n",
        "\n",
        "#nadam_update_parameters\n",
        "def update_params_nadam(params, bracterm, v_bar, epsilon, alpha):\n",
        "    updated_params = {}\n",
        "    num_layers = len(params) // 2\n",
        "    for i in range(1, num_layers + 1):\n",
        "        updated_params[f'W{i}'] = params[f'W{i}'] - alpha/(np.sqrt(v_bar[f'dW{i}'])+epsilon) *   bracterm[f'dW{i}']\n",
        "        updated_params[f'b{i}'] = params[f'b{i}'] - alpha/(np.sqrt(v_bar[f'db{i}'])+epsilon) *   bracterm[f'db{i}']\n",
        "    return updated_params\n",
        "\n",
        "#nesterov_look_ahead\n",
        "def nesterov_params_look(params, u, beta):\n",
        "    updated_params = {}\n",
        "    num_layers = len(params) // 2\n",
        "    for i in range(1, num_layers + 1):\n",
        "        updated_params[f'W{i}'] = params[f'W{i}'] - beta * u[f'dW{i}']\n",
        "        updated_params[f'b{i}'] = params[f'b{i}'] - beta * u[f'db{i}']\n",
        "    return updated_params\n",
        "\n",
        "#nesterov_look_ahead\n",
        "def del_w(u,v,gradients,epsilon):\n",
        "    updated_delw = {}\n",
        "    num_layers = len(params) // 2\n",
        "    for i in range(1, num_layers + 1):\n",
        "            updated_delw[f'dW{i}'] = - (np.sqrt(u[f'dW{i}']+epsilon)/np.sqrt(v[f'dW{i}']+epsilon)) * gradients[f'dW{i}']\n",
        "            updated_delw[f'db{i}'] = - (np.sqrt(u[f'db{i}']+epsilon)/np.sqrt(v[f'db{i}']+epsilon)) * gradients[f'db{i}']\n",
        "    return updated_delw"
      ],
      "metadata": {
        "id": "SUWzlSATVxNm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Traning the model**"
      ],
      "metadata": {
        "id": "0SurYGVZV09e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs=5\n",
        "no_hidden_layers=3\n",
        "size_of_hidden_layer=32\n",
        "weight_decay = 0.\n",
        "alpha=1e-3\n",
        "batch_size=32\n",
        "weight='glorot' #glorot,random\n",
        "activation='relu' #sigmoid,tanh,relu\n",
        "\n",
        "input_size,output_size = 784,10\n",
        "layer_sizes = [input_size] + [size_of_hidden_layer] * no_hidden_layers + [output_size]\n",
        "beta=0.9\n",
        "epsilon=1e-6\n",
        "beta1=0.9\n",
        "beta2=0.999\n",
        "\n",
        "def train_model(opt):\n",
        "  if opt == 'sgd':\n",
        "    params = stochastic_gradient_descent(weight, activation, weight_decay, X_train, Y_train, X_val, Y_val, max_epochs, alpha, layer_sizes, batch_size)\n",
        "  elif opt == 'momentum':\n",
        "    params = momentum_gradient_descent(weight, activation, weight_decay, X_train, Y_train, X_val, Y_val, max_epochs, alpha, beta, layer_sizes, batch_size)\n",
        "  elif opt == 'nesterov':\n",
        "    params = nesterov_gradient_descent(weight, activation, weight_decay, X_train, Y_train, X_val, Y_val, max_epochs, alpha, beta, layer_sizes, batch_size)\n",
        "  elif opt == 'rmsprop':\n",
        "    params = stochastic_gradient_descent_with_RMSProp(weight, activation, weight_decay, X_train, Y_train, X_val, Y_val,max_epochs, alpha, layer_sizes, batch_size, epsilon, beta)\n",
        "  elif opt == 'adam':\n",
        "    params = stochastic_gradient_descent_with_adam(weight, activation, weight_decay, X_train, Y_train, X_val, Y_val, max_epochs, alpha, layer_sizes, batch_size, epsilon, beta1, beta2)\n",
        "  elif opt == 'nadam':\n",
        "    params = stochastic_gradient_descent_with_nadam(weight, activation, weight_decay, X_train, Y_train, X_val, Y_val, max_epochs, alpha, layer_sizes, batch_size, epsilon, beta1, beta2)\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid optimizer option: {opt}\")\n",
        "\n",
        "  # Accuracy checker:\n",
        "  activations_test = forward_prop(params, X_test, activation)\n",
        "  test = activations_test[f'h{len(layer_sizes)-1}']\n",
        "  reverse_onehat = np.argmax(test, axis=0)\n",
        "  test_accuracy = (np.count_nonzero((reverse_onehat-Y_test) == 0)/len(Y_test))*100\n",
        "  print(\"Accuracy on test data= \" + str(test_accuracy) + \" %\")"
      ],
      "metadata": {
        "id": "sek6KuW2V4Uz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model('momentum')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35vskfwIV8Q4",
        "outputId": "069eab2f-0f47-413f-da52-6d9183a1a28b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5; Batch:  1687/1687; Train Loss: 0.06705850927382326; Val Loss: 0.053947216686166144\n",
            "Epoch: 2/5; Batch:  1687/1687; Train Loss: 0.06280388096658601; Val Loss: 0.04691587561810313\n",
            "Epoch: 3/5; Batch:  1687/1687; Train Loss: 0.06165828695339315; Val Loss: 0.043509580832396075\n",
            "Epoch: 4/5; Batch:  1687/1687; Train Loss: 0.06132715088420857; Val Loss: 0.04146964674878003\n",
            "Epoch: 5/5; Batch:  1687/1687; Train Loss: 0.05873975823696202; Val Loss: 0.039896586536219365\n",
            "Accuracy on test data= 84.46000000000001 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model('adam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbaycq74XN4F",
        "outputId": "38a2063d-cab4-4fea-d0f1-f1d7a0974c96"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5; Batch:  1687/1687; Train Loss: 0.05998754623068191; Val Loss: 0.04308040804055756\n",
            "Epoch: 2/5; Batch:  1687/1687; Train Loss: 0.054587721229956275; Val Loss: 0.03935447301678587\n",
            "Epoch: 3/5; Batch:  1687/1687; Train Loss: 0.05119604347717123; Val Loss: 0.037454337666288556\n",
            "Epoch: 4/5; Batch:  1687/1687; Train Loss: 0.04914348318689472; Val Loss: 0.03626974243782838\n",
            "Epoch: 5/5; Batch:  1687/1687; Train Loss: 0.04627860513528143; Val Loss: 0.035558012213474814\n",
            "Accuracy on test data= 85.88 %\n"
          ]
        }
      ]
    }
  ]
}